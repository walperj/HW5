{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX36AFifgZ5T"
   },
   "source": [
    "3.1\n",
    "\n",
    "For an input of size ci x h x w, a kernel of co x ci x kh x kw, padding (ph,pw) and stride(sh,sw) we can break it down into costs per single value in each of the output channels, then costs per output channel, then total costs.\n",
    "\n",
    "Each output value is the conclusion of multiplying kh x kw input numbers in each input channel and then adding them all together. That is, they require kh x kw multiplications per input channel, meaning kh x kw x ci total multiplications per single output value. The number of additions is thus that number minus one, kh x kw x ci - 1.\n",
    "\n",
    "Each output channel has a size of floor((h+ph-kh+1)/sh,(w+pw-kw+1)/sw), meaning the total number of values in an output channel is floor((h+ph-kh+1)/sh) x floor((w+pw-kw+1)/sw). If there are co output channels this results in a total of co x floor((h+ph-kh+1)/sh) x floor((w+pw-kw+1)/sw) values to be outputted.\n",
    "\n",
    "Thus, the total computational costs are:\n",
    "\n",
    "\n",
    "Multiplication: kh x kw x ci x co x floor((h+ph-kh+1)/sh) x floor((w+pw-kw+1)/sw)\n",
    "\n",
    "Addition: (kh x kw x ci - 1) x co x floor((h+ph-kh+1)/sh) x floor((w+pw-kw+1)/sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K1_3PMPkauZ"
   },
   "source": [
    "3.2\n",
    "\n",
    "The memory footprint of forward propogation is the size of the kernel times 2 ie co x ci x kh x kw x 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh1SslA5yFtq"
   },
   "source": [
    "3.3\n",
    "\n",
    "The memory footprint of the forward propogation is the size of the kernel times 3 (kernel value, gradient and un-activated solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3EzeSeiybYw"
   },
   "source": [
    "3.4\n",
    "\n",
    "The computational cost of the backward propogation is equivalent to the twice the amount of the size of the kernel as each point in the kernel needs to have the gradient calculated, and then has to be altered by an appropriate amount in respect to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SwaHVmcvCQSR",
    "outputId": "70eec649-9dfa-41fa-8d0b-fca05843576e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 0\n",
      "2 1\n",
      "3 2\n",
      "4 2\n",
      "5 3\n",
      "6 3\n",
      "7 3\n",
      "8 3\n",
      "9 4\n",
      "10 4\n",
      "11 4\n",
      "12 4\n",
      "13 4\n",
      "14 4\n",
      "15 4\n",
      "16 4\n",
      "17 5\n",
      "18 5\n",
      "19 5\n",
      "20 5\n",
      "21 5\n",
      "22 5\n",
      "23 5\n",
      "24 5\n",
      "25 5\n",
      "26 5\n",
      "27 5\n",
      "28 5\n",
      "29 5\n",
      "30 5\n",
      "31 5\n",
      "32 5\n",
      "33 6\n",
      "34 6\n",
      "35 6\n",
      "36 6\n",
      "37 6\n",
      "38 6\n",
      "39 6\n",
      "40 6\n",
      "41 6\n",
      "42 6\n",
      "43 6\n",
      "44 6\n",
      "45 6\n",
      "46 6\n",
      "47 6\n",
      "48 6\n",
      "49 6\n",
      "50 6\n",
      "51 6\n",
      "52 6\n",
      "53 6\n",
      "54 6\n",
      "55 6\n",
      "56 6\n",
      "57 6\n",
      "58 6\n",
      "59 6\n",
      "60 6\n",
      "61 6\n",
      "62 6\n",
      "63 6\n",
      "64 6\n",
      "65 7\n",
      "66 7\n",
      "67 7\n",
      "68 7\n",
      "69 7\n",
      "70 7\n",
      "71 7\n",
      "72 7\n",
      "73 7\n",
      "74 7\n",
      "75 7\n",
      "76 7\n",
      "77 7\n",
      "78 7\n",
      "79 7\n",
      "80 7\n",
      "81 7\n",
      "82 7\n",
      "83 7\n",
      "84 7\n",
      "85 7\n",
      "86 7\n",
      "87 7\n",
      "88 7\n",
      "89 7\n",
      "90 7\n",
      "91 7\n",
      "92 7\n",
      "93 7\n",
      "94 7\n",
      "95 7\n",
      "96 7\n",
      "97 7\n",
      "98 7\n",
      "99 7\n"
     ]
    }
   ],
   "source": [
    "# note this is just to check how many max comparisons are needed to reach an exact solution\n",
    "import math\n",
    "for i in range(100):\n",
    "  count = 0\n",
    "  num = i\n",
    "  while num > 1:\n",
    "    num = num - math.floor(num/2)\n",
    "    count = count + 1\n",
    "  print(i, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T15cWWLNLLO-"
   },
   "outputs": [],
   "source": [
    "# first utilize a convolution definition from d2l\n",
    "def corr2d(X, K):\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NqCx0yjy8hGp"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTPTw12MZh1E"
   },
   "source": [
    "4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NREsLMR0Zo3B"
   },
   "outputs": [],
   "source": [
    "def avg_pool(X,ker_size):\n",
    "  kernel = torch.full((ker_size,ker_size),(1/ker_size**2))\n",
    "  return corr2d(X,kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "Sid2fwB9cVWu",
    "outputId": "d61bdfc4-4d2f-4974-df86-41833482aa9a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corr2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c1db4ff152a8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mavgX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavgX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-218472e93b8d>\u001b[0m in \u001b[0;36mavg_pool\u001b[0;34m(X, ker_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mker_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mker_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mker_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mker_size\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mcorr2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corr2d' is not defined"
     ]
    }
   ],
   "source": [
    "X = torch.randint(1,100,(7,7))\n",
    "avgX = avg_pool(X,4)\n",
    "print(X)\n",
    "print(avgX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq4cZtM3c_mf"
   },
   "source": [
    "4.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xBlBChi8RBm"
   },
   "outputs": [],
   "source": [
    "# max pooling\n",
    "\n",
    "# first take an a and b\n",
    "a = torch.tensor([-100])\n",
    "b = torch.tensor([-255])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dzZPjdT8ZsH"
   },
   "outputs": [],
   "source": [
    "aMb = a - b\n",
    "bMa = -aMb\n",
    "aPb = a + b\n",
    "MaPb = -aPb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Wg4odcS8yFf",
    "outputId": "a8eb9723-f2a5-4da3-cf49-1ccdde929ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-100.])\n"
     ]
    }
   ],
   "source": [
    "# if b > a, aMb is - and bMa is +, otherwise bMa is - and aMb is +\n",
    "# Thus if we ReLU both, one will return a 0 and the other will give the difference, we then add these together to get the absolute difference\n",
    "\n",
    "dif = torch.relu(aMb) + torch.relu(bMa)\n",
    "add = torch.relu(aPb) - torch.relu(MaPb)\n",
    "\n",
    "# now we have the absolute difference of the two numbers, if we add the product of these numbers we get as a resultant twice the amount of the larger number (assuming both are positive)\n",
    "# assume a - b + a + b = 2*a\n",
    "\n",
    "max = (dif + add)/2\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poYBrlNxdC2x"
   },
   "source": [
    "4.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQtTPgf9_3_-"
   },
   "outputs": [],
   "source": [
    "# therefore an exact max function would be done using the following function\n",
    "import torch\n",
    "def my_max(a,b):\n",
    "  ta = torch.tensor([a])\n",
    "  tb = torch.tensor([b])\n",
    "  return (torch.relu(ta-tb)+torch.relu(tb-ta)+ta+tb)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBAgxVVMAj5i",
    "outputId": "c0fb009e-f9a8-4266-d9fc-8834e131a2e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_max(-100,-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dve-UC_CAva4",
    "outputId": "2a550599-bb39-40b9-b68b-d54a6b6f094c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19., 20.],\n",
      "        [24., 25.]])\n"
     ]
    }
   ],
   "source": [
    "# assuming no stride and an n x n grid\n",
    "# 2 x 2 comparisons would use a convolution of torch.tensor([[0,1],[-1,0]]) and torch.tensor([[0,-1],[1,0]]) and , which would be ReLU-ed then summed with\n",
    "# convolution of torch.tensor([[0,1],[1,0]]) well as a\n",
    "# torch.tensor([[1,0],[0,-1]]) and torch.tensor([[-1,0],[0,1]]), which would be ReLU-ed and summed, with a convolution of torch.tensor([[0,1],[1,0]]).\n",
    "# These should somehow be compared with each other.\n",
    "\n",
    "# to do this we combine the two previous resultants into either one tensor, alternating each value in each row like ([[1t11,2t11,1t12,2t12,...],[1t21,2t21,1t22,2t22,...],...])\n",
    "# then pass two convolutions of stride 2 and torch.tensor([1,-1]) and torch.tensor([-1,1]) which would then be ReLU-ed and summed along with an original convolution of\n",
    "# stride 2, torch.tensor([1,1])\n",
    "\n",
    "# similarly perhaps, we could do a 1x1 convolution with the two seperate solutions as channels and merely use the convolutions of [-1],[1] and [1],[-1] then ReLU-ed and summed,\n",
    "# with an addition of a [1],[1] convolution Though this would add convusion in a space of many many channels, thus use the original idea\n",
    "\n",
    "def max_2x2(in_var):\n",
    "  con11 = torch.relu(corr2d(in_var,torch.tensor([[0,1],[-1,0]])))\n",
    "  con12 = torch.relu(corr2d(in_var,torch.tensor([[0,-1],[1,0]])))\n",
    "  con13 = torch.relu(corr2d(in_var,torch.tensor([[0,1],[1,0]])))\n",
    "  con14 = torch.relu(corr2d(in_var,torch.tensor([[0,-1],[-1,0]])))\n",
    "  con21 = torch.relu(corr2d(in_var,torch.tensor([[1,0],[0,-1]])))\n",
    "  con22 = torch.relu(corr2d(in_var,torch.tensor([[-1,0],[0,1]])))\n",
    "  con23 = torch.relu(corr2d(in_var,torch.tensor([[1,0],[0,1]])))\n",
    "  con24 = torch.relu(corr2d(in_var,torch.tensor([[-1,0],[0,-1]])))\n",
    "  con1 = (con11 + con12 + con13 - con14)/2\n",
    "  con2 = (con21 + con22 + con23 - con24)/2\n",
    "  return (torch.relu(con1 - con2) + torch.relu(con2 - con1) + torch.relu(con1 + con2) - torch.relu(-con1 - con2))/2\n",
    "def max_3x3(in_var):\n",
    "  return(max_2x2(max_2x2(in_var)))\n",
    "\n",
    "X = torch.tensor([[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20],[21,22,23,24,25]])\n",
    "max = max_2x2(X)\n",
    "max = max_3x3(max)\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrWXLpwFdF8q"
   },
   "source": [
    "4.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bErhe-3jLpZH"
   },
   "source": [
    "A 2x2 convolution/ReLU max pooling requires four layers, one to make two seperate replicas of the initial with 2 x 1 channels, one of 4 x 2 channels and one of 2 x 4 channels, and then one of 1 x 2 channels.\n",
    "\n",
    "A 3x3 max pooling would just need for two of these to be done. ie 8 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_gFkonme8qh"
   },
   "source": [
    "4.3\n",
    "\n",
    "For c chanels, (h,w) input size, (p_h,p_w) padding, (p_h,p_w) pooling kernel size and (s_h,s_w) strides we can calculate the outcome's shape.\n",
    "\n",
    "The output size will be floor((h+p_h-p_h+1)/s_h,(w+p_w-p_w+1)/s_w) or floor(h+1/s_h,w+1/s_w). This results in a total of floor(h+1/s_h)*floor(w+1/s_w) total pooling instances, which means the entire computational cost is this number times the computational cost for one case of pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ADiSkB_gQfy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
